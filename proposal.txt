Cortical Learning Algorithms
COS 511 Project Proposal
Cathy Chen and Stefan Keselj

We will analyze the neuroidal model, a biologically plausible computational
model proposed by Leslie Valiant in 1994. The model consists of a set of
neurons connected to eachother by directional synapses to form a DAG. When
stimuli are given to the lowest members of the DAG (topologically), they
are carried by the synapses to other neurons, which also fire if the sum
of their incoming stimuli passes a threshold. Ideally, the highest members
of the DAG will mean something. For example, people have managed to do
supervised learning of classification using this model. If this sounds
similar to the neural nets that are so popular now it's because it is. This
model is the same as a vanilla feedfoward network except for a crucial
difference in character: this model is more discrete and flexible. This
makes the model more interesting to biologists because it's more realistic,
less interesting to engineers because it's less practical, and interesting
to us theorists because it will be easier to analyze than a vanilla net.
In this model we are thinking about sparse 1's and 0's, but in the
mainstream model we would have to deal with dense continuous values. It
is our hope that we will be able to apply some of the tools from this
class to this setting. Specifically, we would like to see what can be
said about the hypothesis classes of different flavors of neuroidal models.
(seems relatively straightforward using our theorems surrounding VC-dim
and rademacher complexity) and if any guarantees could be given about
learning (seems relatively hard, but perhaps we could dumb the model down
enough to say something). This would be useful because it would give
people a ballpark idea about the representational power and learning
capabilities of this networks. If these are bad, we will know to look
for other models. If they are promising, people building neural nets
might be able to use our analysis as a guide.
