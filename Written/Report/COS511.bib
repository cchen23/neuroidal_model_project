
@article{papadimitriou_cortical_2015,
	title = {Cortical {Learning} via {Prediction}},
	volume = {40},
	abstract = {What is the mechanism of learning in the brain? Despite breathtaking advances in neuroscience, and in machine learning, we do not seem close to an answer. Using Valiant’s neuronal model as a foundation, we introduce PJOIN (for “predictive join”), a primitive that combines association and prediction. We show that PJOIN can be implemented naturally in Valiant’s conservative, formal model of cortical computation. Using PJOIN — and almost nothing else — we give a simple algorithm for unsupervised learning of arbitrary ensembles of binary patterns (solving an open problem in Valiant’s work). This algorithm relies crucially on prediction, and entails signiﬁcant downward trafﬁc (“feedback”) while parsing stimuli. Prediction and feedback are well-known features of neural cognition and, as far as we know, this is the ﬁrst theoretical prediction of their essential role in learning.},
	language = {en},
	journal = {JMLR: Workshop and Conference Proceedings},
	author = {Papadimitriou, Christos H and Vempala, Santosh S},
	year = {2015},
	pages = {21},
	file = {Papadimitriou and Vempala - Cortical Learning via Prediction.pdf:C\:\\Users\\Cathy\\Zotero\\storage\\X2BUJRF3\\Papadimitriou and Vempala - Cortical Learning via Prediction.pdf:application/pdf}
}

@article{valiant_neuroidal_2000,
	title = {A {Neuroidal} {Architecture} for {Cognitive} {Computation}},
	volume = {47},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/355483.355486},
	doi = {10.1145/355483.355486},
	abstract = {An architecture is described for designing systems that acquire and ma nipulate large amounts of unsystematized, or so-called commonsense, knowledge. Its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases. The architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make this computationally tractable even for very large databases. The main claims are that (i) the basic learning and deduction tasks are provably tractable and (ii) tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are programmed. Among the issues that learning offers to resolve are robustness to inconsistencies, robustness to incomplete information and resolving among alternatives. Attribute-efficient learning algorithms, which allow learning from few examples in large dimensional systems, are fundamental to the approach. Underpinning the overall architecture is a new principled approach to manipulating relations in learning systems. This approach, of independently quantified arguments, allows propositional learning algorithms to be applied systematically to learning relational concepts in polynomial time and in modular fashion.},
	number = {5},
	urldate = {2018-04-09},
	journal = {J. ACM},
	author = {Valiant, Leslie G.},
	month = sep,
	year = {2000},
	keywords = {cognitive computation, computational learning, learning relations, nonmonotonic reasoning, PAC learning, robust reasoning},
	pages = {854--882},
	file = {ACM Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\9CTI48FA\\Valiant - 2000 - A Neuroidal Architecture for Cognitive Computation.pdf:application/pdf}
}

@article{feldman_experience-induced_2009,
	title = {Experience-induced neural circuits that achieve high capacity},
	volume = {21},
	issn = {0899-7667},
	doi = {10.1162/neco.2009.08-08-851},
	abstract = {Over a lifetime, cortex performs a vast number of different cognitive actions, mostly dependent on experience. Previously it has not been known how such capabilities can be reconciled, even in principle, with the known resource constraints on cortex, such as low connectivity and low average synaptic strength. Here we describe neural circuits and associated algorithms that respect the brain's most basic resource constraints and support the execution of high numbers of cognitive actions when presented with natural inputs. Our circuits simultaneously support a suite of four basic kinds of task, each requiring some circuit modification: hierarchical memory formation, pairwise association, supervised memorization, and inductive learning of threshold functions. The capacity of our circuits is established by experiments in which sequences of several thousand such actions are simulated by computer and the circuits created tested for subsequent efficacy. Our underlying theory is apparently the only biologically plausible systems-level theory of learning and memory in cortex for which such a demonstration has been performed, and we argue that no general theory of information processing in the brain can be considered viable without such a demonstration.},
	language = {eng},
	number = {10},
	journal = {Neural Computation},
	author = {Feldman, Vitaly and Valiant, Leslie G.},
	month = oct,
	year = {2009},
	pmid = {19635015},
	keywords = {Humans, Learning, Memory, Models, Neurological, Neural Pathways, Cerebral Cortex, Animals, Computer Simulation, Neurons, Algorithms, Nerve Net, Neural Networks (Computer), Action Potentials, Cognition, Synaptic Transmission},
	pages = {2715--2754}
}

@article{valiant_memorization_2005,
	title = {Memorization and association on a realistic neural model},
	volume = {17},
	issn = {0899-7667},
	doi = {10.1162/0899766053019890},
	abstract = {A central open question of computational neuroscience is to identify the data structures and algorithms that are used in mammalian cortex to support successive acts of the basic cognitive tasks of memorization and association. This letter addresses the simultaneous challenges of realizing these two distinct tasks with the same data structure, and doing so while respecting the following four basic quantitative parameters of cortex: the neuron number, the synapse number, the synapse strengths, and the switching times. Previous work has not succeeded in reconciling these opposing constraints, the low values of synapse strengths that are typically observed experimentally having contributed a particular obstacle. In this article, we describe a computational scheme that supports both memory formation and association and is feasible on networks of model neurons that respect the widely observed values of the four quantitative parameters. Our scheme allows for both disjoint and shared representations. The algorithms are simple, and in one version both memorization and association require just one step of vicinal or neighborly influence. The issues of interference among the different circuits that are established, of robustness to noise, and of the stability of the hierarchical memorization process are addressed. A calculus therefore is implied for analyzing the capabilities of particular neural systems and subsystems, in terms of their basic numerical parameters.},
	language = {eng},
	number = {3},
	journal = {Neural Computation},
	author = {Valiant, Leslie G.},
	month = mar,
	year = {2005},
	pmid = {15802006},
	keywords = {Humans, Memory, Models, Neurological, Cerebral Cortex, Animals, Computer Simulation, Synapses, Neurons, Algorithms, Neural Networks (Computer), Time Factors, Cell Count},
	pages = {527--555}
}

@book{valiant_circuits_1994,
	address = {New York, NY, USA},
	title = {Circuits of the {Mind}},
	isbn = {978-0-19-508926-4},
	publisher = {Oxford University Press, Inc.},
	author = {Valiant, Leslie G.},
	year = {1994}
}

@article{valiant_quantitative_2006,
	title = {A {Quantitative} {Theory} of {Neural} {Computation}},
	volume = {95},
	issn = {0340-1200},
	url = {http://dx.doi.org/10.1007/s00422-006-0079-3},
	doi = {10.1007/s00422-006-0079-3},
	abstract = {We show how a general quantitative theory of neural computation can be used to explain two recent experimental findings in neuroscience. The first of these findings is that in human medial temporal lobe there exist neurons that correspond to identifiable concepts, such as a particular actress. Further, even when such concepts are preselected by the experimenter, such neurons can be found with paradoxical ease, after examining relatively few neurons. We offer a quantitative computational explanation of this phenomenon, where apparently none existed before. Second, for the locust olfactory system estimates of the four parameters of neuron numbers, synapse numbers, synapse strengths, and the numbers of neurons that represent an odor are now available. We show here that these numbers are related as predicted by the general theory. More generally, we identify two useful regimes for neural computation with distinct ranges of these quantitative parameters.},
	number = {3},
	urldate = {2018-05-08},
	journal = {Biol. Cybern.},
	author = {Valiant, Leslie G.},
	month = aug,
	year = {2006},
	pages = {205--211}
}

@article{quiroga_invariant_2005,
	title = {Invariant visual representation by single neurons in the human brain},
	volume = {435},
	copyright = {2005 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature03687},
	doi = {10.1038/nature03687},
	abstract = {It takes a fraction of a second to recognize a person or an object even when seen under strikingly different conditions. How such a robust, high-level representation is achieved by neurons in the human brain is still unclear1,2,3,4,5,6. In monkeys, neurons in the upper stages of the ventral visual pathway respond to complex images such as faces and objects and show some degree of invariance to metric properties such as the stimulus size, position and viewing angle2,4,7,8,9,10,11,12. We have previously shown that neurons in the human medial temporal lobe (MTL) fire selectively to images of faces, animals, objects or scenes13,14. Here we report on a remarkable subset of MTL neurons that are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names. These results suggest an invariant, sparse and explicit code, which might be important in the transformation of complex visual percepts into long-term and more abstract memories.},
	language = {en},
	number = {7045},
	urldate = {2018-05-09},
	journal = {Nature},
	author = {Quiroga, R. Quian and Reddy, L. and Kreiman, G. and Koch, C. and Fried, I.},
	month = jun,
	year = {2005},
	pages = {1102--1107},
	file = {Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\3DFE74K4\\Quiroga et al. - 2005 - Invariant visual representation by single neurons .pdf:application/pdf;Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\Y8M5VSYX\\nature03687.html:text/html}
}

@article{takeuchi_synaptic_2014,
	title = {The synaptic plasticity and memory hypothesis: encoding, storage and persistence},
	volume = {369},
	issn = {0962-8436},
	shorttitle = {The synaptic plasticity and memory hypothesis},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3843897/},
	doi = {10.1098/rstb.2013.0288},
	abstract = {The synaptic plasticity and memory hypothesis asserts that activity-dependent synaptic plasticity is induced at appropriate synapses during memory formation and is both necessary and sufficient for the encoding and trace storage of the type of memory mediated by the brain area in which it is observed. Criteria for establishing the necessity and sufficiency of such plasticity in mediating trace storage have been identified and are here reviewed in relation to new work using some of the diverse techniques of contemporary neuroscience. Evidence derived using optical imaging, molecular-genetic and optogenetic techniques in conjunction with appropriate behavioural analyses continues to offer support for the idea that changing the strength of connections between neurons is one of the major mechanisms by which engrams are stored in the brain.},
	number = {1633},
	urldate = {2018-05-10},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Takeuchi, Tomonori and Duszkiewicz, Adrian J. and Morris, Richard G. M.},
	month = jan,
	year = {2014},
	pmid = {24298167},
	pmcid = {PMC3843897},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\2PSEL87V\\Takeuchi et al. - 2014 - The synaptic plasticity and memory hypothesis enc.pdf:application/pdf}
}

@article{cooper_donald_2005,
	title = {Donald {O}. {Hebb}'s synapse and learning rule: a history and commentary},
	volume = {28},
	issn = {0149-7634},
	shorttitle = {Donald {O}. {Hebb}'s synapse and learning rule},
	doi = {10.1016/j.neubiorev.2004.09.009},
	abstract = {This year sees the anniversary of Donald O. Hebb's birth, in July 1904. The impact of his work, especially through his neurophysiological postulate, as described in his magnum opus, The organization of behaviour (1949), has been profound in contemporary neuroscience. Hebb's life, and the scientific milieu in psychology and neurophysiology which preceded and informed Hebb's work are described. His core postulate, which gave rise to such eponymous expressions as the Hebbian synapse and the Hebbian learning rule, is examined in some detail, as well as the part it played in his higher-order theoretical constructs concerned with neocortical structure and function. Early models which made use of the Hebbian synapse are described, and then illustrative examples are given detailing the impact of Hebb's idea in relation to learning and memory, synaptic plasticity and stability, and the question of persistent cortical activity underlying forms of short-term memory.},
	language = {eng},
	number = {8},
	journal = {Neuroscience and Biobehavioral Reviews},
	author = {Cooper, Steven J.},
	month = jan,
	year = {2005},
	pmid = {15642626},
	keywords = {Humans, Learning, Models, Neurological, Animals, Synapses, History, 19th Century, History, 20th Century, Neuronal Plasticity, Neuropsychology, Neurosciences},
	pages = {851--874}
}

@article{paulsen_model_1998,
	title = {A model of hippocampal memory encoding and retrieval: {GABAergic} control of synaptic plasticity},
	volume = {21},
	issn = {0166-2236, 1878-108X},
	shorttitle = {A model of hippocampal memory encoding and retrieval},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(97)01205-8},
	doi = {10.1016/S0166-2236(97)01205-8},
	language = {English},
	number = {7},
	urldate = {2018-05-10},
	journal = {Trends in Neurosciences},
	author = {Paulsen, Ole and Moser, Edvard},
	month = jul,
	year = {1998},
	pmid = {9683315},
	keywords = {back propogation, hippocampus, inhibition, interneurones, learning, LTP, phasing, theta},
	pages = {273--278},
	file = {Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\UBHUEJT7\\Paulsen and Moser - 1998 - A model of hippocampal memory encoding and retriev.pdf:application/pdf;Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\47A2Y62V\\S0166-2236(97)01205-8.html:text/html}
}

@article{treves_computational_1992,
	title = {Computational constraints suggest the need for two distinct input systems to the hippocampal {CA}3 network},
	volume = {2},
	issn = {1050-9631},
	doi = {10.1002/hipo.450020209},
	abstract = {The CA3 network in the hippocampus may operate as an autoassociator, in which declarative memories, known to be dependent on hippocampal processing, could be stored, and subsequently retrieved, using modifiable synaptic efficacies in the CA3 recurrent collateral system. On the basis of this hypothesis, the authors explore the computational relevance of the extrinsic afferents to the CA3 network. A quantitative statistical analysis of the information that may be relayed by such afferent connections reveals the need for two distinct systems of input synapses. The synapses of the first system need to be strong (but not associatively modifiable) in order to force, during learning, the CA3 cells into a pattern of activity relatively independent of any inputs being received from the recurrent collaterals, and which thus reflects sizable amounts of new information. It is proposed that the mossy fiber system performs this function. A second system, with a large number of associatively modifiable synapses on each receiving cell, is needed in order to relay a signal specific enough to initiate the retrieval process. This may be identified, we propose, with the perforant path input to CA3.},
	language = {eng},
	number = {2},
	journal = {Hippocampus},
	author = {Treves, A. and Rolls, E. T.},
	month = apr,
	year = {1992},
	pmid = {1308182},
	keywords = {Hippocampus, Humans, Memory, Models, Neurological, Animals, Mathematics, Synapses, Nerve Net, Nerve Fibers, Pyramidal Tracts},
	pages = {189--199}
}

@article{schacter_richard_1978,
	title = {Richard {Semon}'s theory of memory},
	volume = {17},
	issn = {0022-5371},
	url = {http://www.sciencedirect.com/science/article/pii/S0022537178904437},
	doi = {10.1016/S0022-5371(78)90443-7},
	abstract = {In the first decade of the 20th century, Richard Semon put forward a theory of memory that anticipated numerous recent developments in memory research. The theory is discussed both in its historical context and with reference to modern ideas. Semon's theoretical concern for retrieval phenomena is particularly noteworthy. Several reasons are suggested why the theory is virtually unknown today.},
	number = {6},
	urldate = {2018-05-11},
	journal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Schacter, Daniel L. and Eich, James Eric and Tulving, Endel},
	month = dec,
	year = {1978},
	pages = {721--743},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\SXUYMFDH\\Schacter et al. - 1978 - Richard Semon's theory of memory.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\GGBGM2NN\\S0022537178904437.html:text/html}
}

@article{purves_refractory_2001,
	title = {The {Refractory} {Period}},
	url = {https://www.ncbi.nlm.nih.gov/books/NBK11146/},
	abstract = {The depolarization that produces Na+ channel opening also causes delayed activation of K+ channels and Na+ channel inactivation, leading to repolarization of the membrane potential as the action potential sweeps along the length of an axon (see Figure 3.12). In its wake, the action potential leaves the Na+ channels inactivated and K+ channels activated for a brief time. These transitory changes make it harder for the axon to produce subsequent action potentials during this interval, which is called the refractory period. Thus, the refractory period limits the number of action potentials that a given nerve cell can produce per unit time. As might be expected, different types of neurons have different maximum rates of action potential firing due to different types and densities of ion channels. The refractoriness of the membrane in the wake of the action potential explains why action potentials do not propagate back toward the point of their initiation as they travel along an axon.},
	language = {en},
	urldate = {2018-05-13},
	author = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Katz, Lawrence C. and LaMantia, Anthony-Samuel and McNamara, James O. and Williams, S. Mark},
	year = {2001},
	file = {Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\PFW94U29\\NBK11146.html:text/html}
}