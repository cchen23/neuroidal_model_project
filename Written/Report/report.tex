\documentclass[letterpaper, 12pt]{article}

\usepackage[normalem]{ulem}
\usepackage{scrextend}	% for the indented environment
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{graphicx, caption, subcaption}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

% make everything 12pt font
\let\Huge\normalsize
\let\huge\normalsize
\let\LARGE\normalsize
\let\Large\normalsize
\let\large\normalsize
\let\small\normalsize
\let\footnotesize\normalsize
\let\scriptsize\normalsize
\let\tiny\normalsize

% Make everything single spacing
\linespread{1.0}

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
\textbf{Study and Comparison of the Neuroidal Model} \hfill \newline Cathy Chen and Stefan Keselj \\
COS511 Final Project Report \\
Due May 15, 2018

\section{Introduction}
We studied the neuroidal model, a biologically plausible model of learning proposed by Valiant in 1994 \cite{valiant_circuits_1994}. In this report we present a concise summary of the model and abilities, based on Valiant's original book and subsequent work. We then study an extension proposed by Papadimitriou and Vempala, and describe the benefits of this extension \cite{papadimitriou_cortical_2015}. We select a few algorithms from these works to implement, and compare mechanisms of the neuroidal model to hypotheses about the human brain and to modern artificial neural networks.

\section{Neuroidal Model}\label{sec:model}
In this section we summarize our understanding of the neuroidal model. Our understanding primarily draws from \cite{valiant_circuits_1994, valiant_memorization_2005, papadimitriou_cortical_2015}.

The neuroidal model consists of a system of \underline{neurons} and \underline{synapses} which can be modeled as a weighted directed graph in which nodes represent neurons and edges represent synapses. Each neuron $i$ has a state $s_i$ containing three variables: a threshold $T\in\mathbb{R}_{>0}$, a categorical memory variable $q$, and an indicator variable $f$ that says whether the network is firing at that timestep. (In more complex versions of the model, $T\in\mathbb{R}^\gamma$, for $\gamma\in\mathbb{Z}$, which allows each neuron to store more information.) The synapse from neuron $i$ to neuron $j$ is represented by $w_{ji}\in W$, where $W$ may be a set such as $\mathbb{R}$ or $\{0,1\}$. In some versions of the model, each synapse also contains a memory state $qq$.

The system operates with discrete timesteps and all neuroid and synapse states update at each timestep according to predefined functions. This model uses \underline{vicinal} algorithms, meaning that these predefined functions are local: each neuron's update depends only on its own state, the firing of adjacent neurons, and the weights connecting the neuron and its neighbors. More specifically, the functions have the form $s_{i,t+1}=\delta(s_{i,t},w_i)$, $w_{ji,t+1}=\lambda(s_i,w_i,w_{ji},f_j)$, where $w_i$ is the sum over all $w_{ki}$ for all firing neurons $k$.

Each conceptual ``item" is represented by a group of $r$ neurons. The firing of an item's neurons denotes that the activation of the item (in terms of cognition, this means the system is ``thinking about" this item).

In \cite{valiant_circuits_1994} Valiant describes four modes of learning that result from two dichotomies: one between memorization () and inductive learning (), and the other between supervised () and unsupervised () learning. He describes algorithms that perform all four modes of learning with the neuroidal model. 

- TODO: MORE INFO ABOUT ALGORITHMS FOR FOUR MODES. PAC LEARNING FOR INDUCTIVE LEARNING\\
- JOIN: FORMING BOOLEAN CONJUNCTIONS OF ATTRIBUTES (UNSUPERVISED MEM); LINK (SUPERVISED MEM); LEARNING FUNCTIONS (SUPERVISED INDUCTIVE MEM); CORRELATIONAL LEARNING (UNSUPERVISED INDUCTIVE MEM);

\section{Selected Algorithms}\label{sec:selected_algorithms}
Our study focused on the JOIN and LINK algorithms, which respectively implement unsupervised and supervised memorization (in terms of Valiant's dichotomies) and forming memories and associations (in terms of cognitive functions) \cite{valiant_circuits_1994, papadimitriou_cortical_2015}. In this section, we TODO:DESCRIBE ANALYSES, TODO:OUR IMPLEMENTATION of these functions, and an extension of Valiant's work the proposed a new function.

We implement the algorithms according to specifics described in \cite{valiant_memorization_2005}.
- ASSUME BIPARTITE GRAPHS (p550)\\
- link to code\\
- simulations\\
- compare to bounds?\\
- bounds\\

\subsection{Extension by Papadimitriou and Vempala}\label{sec:pjoin}
Papadimitriou and Vempala \cite{papadimitriou_cortical_2015} extend Valiant's work, adding an additional PJOIN operation.
- motivation for operation\\
- concise description of PJOIN\\
- bounds?\\

\section{Comparison to Human Brains}
Valiant's original model respects constraints posed by the human brain, such as the sparsity of connections and speed of processing in the human brain, and his comparison to experimental data from a biological system validates the biological plausibility of the neuroidal model \cite{valiant_quantitative_2006}. In this section we compare in more detail the neuroidal model and current hypotheses about learning in the human brain, particularly with respect to memory.

- grandmother cell,  jennifer anniston cell \cite{quiroga_invariant_2005}.\\
- compare to hopfield nets?\\
- neurons/synapses/updates\\
- action potentials (threshold)\\
- memory (esp associative memory) coactivation, random initialization\\
- learning and execution\\
	- different modes of hippocampus\\

- pattern separation/sparse activation (mtl part 1)\\
- item recognition in perirhinal cortex % https://piazza-resources.s3.amazonaws.com/iyg3i7tq28t7nt/iz78zxzjdb38s/MTL_PART_TWO_2017_REVISED.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAJZDMPX3EN6BQWN6Q%2F20180509%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180509T001944Z&X-Amz-Expires=10800&X-Amz-SignedHeaders=host&X-Amz-Security-Token=FQoDYXdzEMj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDDXb%2FAIhvnnG4RNlFiK3A7wcfEeEN%2B8laZxFcFsojjJfa46YQ4UkSwWrnEk%2B9IWfpipzsoBL9F0tNM%2FlK%2FSu3mowSWz869pX%2FnZJGrYJBqn%2B6i4EI7Vw1thuc67kqNYGQOmznxfig9ekR2TP%2Blz2vmoxb771Dd23fMgO98JJex2OTVpkeiG7TrYFe6VCmTUM8nlGeXmxamCGGr%2F%2BGtfSn3nnnIpKJa02p%2BFw1fWEYIcMSAw1ngL9o25ll9161%2FFs7WOxUvThHsSuEYGoc1l0qzrlhVcfCU9EddlmbFerSX4xxfG1926wfntVTxNJu%2FqdjKM2%2FTcm%2B9qo0QgzWt83tsODIR14JacR8uwDW%2FUfOjomWJYbGkGXvOCLUR0aju%2BMRsmkwkhMAAuoS1UK%2ByvFBue50NXyvfSNKR5owNCz%2B70DoX0WqbsbuAO5iDdlx%2B3jebtAjPOmV366S5WW0nBDMF88VMMWid9Nb0VTl%2BDMzAlbF56rZDz0U2Mvoxf2E%2Bb39MQwBKTKXURqAVXQAv7PCKj1hweVAK0XCYU6JF39RARoapDjxkJEUn%2FSVKDddbGPi0Gn5WBWzS%2BxS%2FPQRJmnbzzH4UOvdoAo18rI1wU%3D&X-Amz-Signature=afa591a669c54b99660f738b80d21fb725f462c1a9449bb846dd4f0e9a4bf9bd

\section{Comparison to Neural Networks}
The neuroidal model shares many structural similarities with neural networks, yet recent literature surrounding this model has been much sparser than than that of neural networks. In this section we compare structural and functional facets of the two models.

- similarities in structure\\
- differences in structure\\
- benefits\\
	- more interpretable (explicitly designing algorithms)\\
- detriments\\
	- hard to use as a black box; need explicit algorithms\\

\section{Conclusion}
Rather than perform neurobiological studies or behavioral experiments, Valiant proposes a computational approach of building a system that performs the desired behavior using components inspired by biology \cite{valiant_circuits_1994}. With this approach, the neuroidal model differs from work with the brain and artificial neural networks.

Unlike studies of the brain that start from specific analyses of biological components, this approach uses computational constraints to limit the space of algorithms, and finds algorithms within this space that accomplishes desired functions.

Unlike artificial neural networks which learn algorithms that are generally hard to interpret, the neuroidal model requires designing ``explicit computational mechanisms" for ``explicitly defined and possibly multiple cognitive tasks"; the need for designing \textit{explicit} mechanisms and tasks makes the neuroidal model perhaps less useful (at least at its current state) for generally solving problems, a use for which artificial neural networks have received much attention in the past few years.

Though this provides a disadvantage in terms of engineering solutions to large-scale problems, the explicit nature of the neuroidal model serves as an advantage for forming scientific hypotheses about learning. The explicit structure of the model and algorithms make the neuroidal model amenable to analyses of system parameters and capacity bounds that we mention in Section \ref{sec:model}, and the explicit nature of algorithms that we describe and implement in Section \ref{sec:selected_algorithms} allows us to more directly map the capabilities of the neuroidal model to hypotheses about how learning occurs in the human brain.

\bibliographystyle{IEEEtranS}
\bibliography{COS511}

\end{document}
