
@article{papadimitriou_cortical_2015,
	title = {Cortical {Learning} via {Prediction}},
	volume = {40},
	abstract = {What is the mechanism of learning in the brain? Despite breathtaking advances in neuroscience, and in machine learning, we do not seem close to an answer. Using Valiant’s neuronal model as a foundation, we introduce PJOIN (for “predictive join”), a primitive that combines association and prediction. We show that PJOIN can be implemented naturally in Valiant’s conservative, formal model of cortical computation. Using PJOIN — and almost nothing else — we give a simple algorithm for unsupervised learning of arbitrary ensembles of binary patterns (solving an open problem in Valiant’s work). This algorithm relies crucially on prediction, and entails signiﬁcant downward trafﬁc (“feedback”) while parsing stimuli. Prediction and feedback are well-known features of neural cognition and, as far as we know, this is the ﬁrst theoretical prediction of their essential role in learning.},
	language = {en},
	journal = {JMLR: Workshop and Conference Proceedings},
	author = {Papadimitriou, Christos H and Vempala, Santosh S},
	year = {2015},
	pages = {21},
	file = {Papadimitriou and Vempala - Cortical Learning via Prediction.pdf:C\:\\Users\\Cathy\\Zotero\\storage\\X2BUJRF3\\Papadimitriou and Vempala - Cortical Learning via Prediction.pdf:application/pdf}
}

@article{valiant_neuroidal_2000,
	title = {A {Neuroidal} {Architecture} for {Cognitive} {Computation}},
	volume = {47},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/355483.355486},
	doi = {10.1145/355483.355486},
	abstract = {An architecture is described for designing systems that acquire and ma nipulate large amounts of unsystematized, or so-called commonsense, knowledge. Its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases. The architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make this computationally tractable even for very large databases. The main claims are that (i) the basic learning and deduction tasks are provably tractable and (ii) tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are programmed. Among the issues that learning offers to resolve are robustness to inconsistencies, robustness to incomplete information and resolving among alternatives. Attribute-efficient learning algorithms, which allow learning from few examples in large dimensional systems, are fundamental to the approach. Underpinning the overall architecture is a new principled approach to manipulating relations in learning systems. This approach, of independently quantified arguments, allows propositional learning algorithms to be applied systematically to learning relational concepts in polynomial time and in modular fashion.},
	number = {5},
	urldate = {2018-04-09},
	journal = {J. ACM},
	author = {Valiant, Leslie G.},
	month = sep,
	year = {2000},
	keywords = {cognitive computation, computational learning, learning relations, nonmonotonic reasoning, PAC learning, robust reasoning},
	pages = {854--882},
	file = {ACM Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\9CTI48FA\\Valiant - 2000 - A Neuroidal Architecture for Cognitive Computation.pdf:application/pdf}
}

@article{feldman_experience-induced_2009,
	title = {Experience-induced neural circuits that achieve high capacity},
	volume = {21},
	issn = {0899-7667},
	doi = {10.1162/neco.2009.08-08-851},
	abstract = {Over a lifetime, cortex performs a vast number of different cognitive actions, mostly dependent on experience. Previously it has not been known how such capabilities can be reconciled, even in principle, with the known resource constraints on cortex, such as low connectivity and low average synaptic strength. Here we describe neural circuits and associated algorithms that respect the brain's most basic resource constraints and support the execution of high numbers of cognitive actions when presented with natural inputs. Our circuits simultaneously support a suite of four basic kinds of task, each requiring some circuit modification: hierarchical memory formation, pairwise association, supervised memorization, and inductive learning of threshold functions. The capacity of our circuits is established by experiments in which sequences of several thousand such actions are simulated by computer and the circuits created tested for subsequent efficacy. Our underlying theory is apparently the only biologically plausible systems-level theory of learning and memory in cortex for which such a demonstration has been performed, and we argue that no general theory of information processing in the brain can be considered viable without such a demonstration.},
	language = {eng},
	number = {10},
	journal = {Neural Computation},
	author = {Feldman, Vitaly and Valiant, Leslie G.},
	month = oct,
	year = {2009},
	pmid = {19635015},
	keywords = {Humans, Learning, Memory, Models, Neurological, Neural Pathways, Cerebral Cortex, Animals, Computer Simulation, Neurons, Algorithms, Nerve Net, Neural Networks (Computer), Action Potentials, Cognition, Synaptic Transmission},
	pages = {2715--2754}
}

@article{valiant_memorization_2005,
	title = {Memorization and association on a realistic neural model},
	volume = {17},
	issn = {0899-7667},
	doi = {10.1162/0899766053019890},
	abstract = {A central open question of computational neuroscience is to identify the data structures and algorithms that are used in mammalian cortex to support successive acts of the basic cognitive tasks of memorization and association. This letter addresses the simultaneous challenges of realizing these two distinct tasks with the same data structure, and doing so while respecting the following four basic quantitative parameters of cortex: the neuron number, the synapse number, the synapse strengths, and the switching times. Previous work has not succeeded in reconciling these opposing constraints, the low values of synapse strengths that are typically observed experimentally having contributed a particular obstacle. In this article, we describe a computational scheme that supports both memory formation and association and is feasible on networks of model neurons that respect the widely observed values of the four quantitative parameters. Our scheme allows for both disjoint and shared representations. The algorithms are simple, and in one version both memorization and association require just one step of vicinal or neighborly influence. The issues of interference among the different circuits that are established, of robustness to noise, and of the stability of the hierarchical memorization process are addressed. A calculus therefore is implied for analyzing the capabilities of particular neural systems and subsystems, in terms of their basic numerical parameters.},
	language = {eng},
	number = {3},
	journal = {Neural Computation},
	author = {Valiant, Leslie G.},
	month = mar,
	year = {2005},
	pmid = {15802006},
	keywords = {Humans, Memory, Models, Neurological, Cerebral Cortex, Animals, Computer Simulation, Synapses, Neurons, Algorithms, Neural Networks (Computer), Time Factors, Cell Count},
	pages = {527--555}
}

@book{valiant_circuits_1994,
	address = {New York, NY, USA},
	title = {Circuits of the {Mind}},
	isbn = {978-0-19-508926-4},
	publisher = {Oxford University Press, Inc.},
	author = {Valiant, Leslie G.},
	year = {1994}
}